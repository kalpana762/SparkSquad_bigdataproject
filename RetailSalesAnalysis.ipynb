{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc146045-40be-4a28-9a70-347957014cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, sum as _sum, col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retail Sales Analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df528c4c-37ec-4d96-a95a-f2b173f5b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------------+----------------+-----------+----------+------+-----+--------+------+----------+\n",
      "|Order ID|Customer Name|        Category|    Sub Category|       City|Order Date|Region|Sales|Discount|Profit|     State|\n",
      "+--------+-------------+----------------+----------------+-----------+----------+------+-----+--------+------+----------+\n",
      "|     OD1|       Harish|    Oil & Masala|         Masalas|    Vellore|11-08-2017| North| 1254|    0.12|401.28|Tamil Nadu|\n",
      "|     OD2|        Sudha|       Beverages|   Health Drinks|Krishnagiri|11-08-2017| South|  749|    0.18| 149.8|Tamil Nadu|\n",
      "|     OD3|      Hussain|     Food Grains|    Atta & Flour| Perambalur|06-12-2017|  West| 2360|    0.21| 165.2|Tamil Nadu|\n",
      "|     OD4|      Jackson|Fruits & Veggies|Fresh Vegetables| Dharmapuri|10-11-2016| South|  896|    0.25|  89.6|Tamil Nadu|\n",
      "|     OD5|      Ridhesh|     Food Grains| Organic Staples|       Ooty|10-11-2016| South| 2355|    0.26|918.45|Tamil Nadu|\n",
      "+--------+-------------+----------------+----------------+-----------+----------+------+-----+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = spark.read.csv(\"Downloads/SupermartSales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows to verify the data\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98775ceb-5f32-41bb-b807-726fe1c44286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e0b303-461c-405c-90fe-1780f350d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|Order Date| OrderDate|\n",
      "+----------+----------+\n",
      "| 8/25/2015|2015-08-25|\n",
      "| 9/12/2017|2017-09-12|\n",
      "| 3/16/2016|2016-03-16|\n",
      "| 10/3/2016|2016-10-03|\n",
      "|  9/9/2015|2015-09-09|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, hour\n",
    "\n",
    "# Parse the \"Order Date\" in the format MM/d/yyyy (without leading zeros for single digits)\n",
    "data = data.withColumn(\"OrderDate\", to_date(\"Order Date\", \"M/d/yyyy\"))\n",
    "\n",
    "# Show the processed data\n",
    "data.select(\"Order Date\", \"OrderDate\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1afd7e-d243-4475-9aef-c8bd110b0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal 1\n",
    "# Identify top-performing products and their seasonal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d1c68ea-9478-4ea4-97cb-bd6ef14b3ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|         Category|TotalSales|\n",
      "+-----------------+----------+\n",
      "|Eggs, Meat & Fish|   2267401|\n",
      "|           Snacks|   2237546|\n",
      "|      Food Grains|   2115272|\n",
      "|           Bakery|   2112281|\n",
      "| Fruits & Veggies|   2100727|\n",
      "|        Beverages|   2085313|\n",
      "|     Oil & Masala|   2038442|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "# Group by ProductName and calculate total sales\n",
    "product_sales = data.groupBy(\"Category\").agg(_sum(\"Sales\").alias(\"TotalSales\"))\n",
    "\n",
    "# Sort by TotalSales in descending order\n",
    "product_sales = product_sales.orderBy(col(\"TotalSales\").desc())\n",
    "\n",
    "# Show the top-performing products\n",
    "product_sales.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb118e02-19ee-455c-be38-2a4066e6cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal 2\n",
    "#Determine peak sales periods and customer purchasing behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7969866f-75e0-4ef6-9ac9-325659ca753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|             Season|TotalSales|\n",
      "+-------------------+----------+\n",
      "|Post-Monsoon/Autumn|   6559363|\n",
      "|            Monsoon|   5258266|\n",
      "|             Summer|   3139353|\n",
      "+-------------------+----------+\n",
      "\n",
      "+-------------------+----------+\n",
      "|Season             |TotalSales|\n",
      "+-------------------+----------+\n",
      "|Post-Monsoon/Autumn|6559363   |\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, when, col, sum as _sum\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming you have already initialized your SparkSession\n",
    "spark = SparkSession.builder.appName(\"SeasonalSalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Convert Order Date to date type (if not already)\n",
    "data = data.withColumn(\"OrderDate\", to_date(\"Order Date\", \"M/d/yyyy\"))\n",
    "\n",
    "# Extract the month from the Order Date\n",
    "data = data.withColumn(\"Month\", month(\"OrderDate\"))\n",
    "\n",
    "# Map months to seasons\n",
    "data = data.withColumn(\n",
    "    \"Season\",\n",
    "    when(col(\"Month\").between(12, 2), \"Winter\")\n",
    "    .when(col(\"Month\").between(3, 5), \"Summer\")\n",
    "    .when(col(\"Month\").between(6, 9), \"Monsoon\")\n",
    "    .otherwise(\"Post-Monsoon/Autumn\")  # For months 10 and 11\n",
    ")\n",
    "\n",
    "# Group by season and calculate the total sales for each season\n",
    "seasonal_sales = data.groupBy(\"Season\").agg(_sum(\"Sales\").alias(\"TotalSales\"))\n",
    "\n",
    "# Sort by TotalSales in descending order to find the peak season\n",
    "seasonal_sales = seasonal_sales.orderBy(\"TotalSales\", ascending=False)\n",
    "\n",
    "# Show the peak sales periods for each season\n",
    "seasonal_sales.show()\n",
    "\n",
    "# Retrieve the season with the highest sales\n",
    "peak_sales = seasonal_sales.limit(1)\n",
    "\n",
    "# Print the peak sales season\n",
    "peak_sales.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba25a7-b718-4180-98a7-faa55e9950ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal 3\n",
    "# Segment customers based on their purchasing behavior, such as purchase frequency, average spending, and product preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f691716e-e133-442f-a95e-f0f758367a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------------+------------------+----------+\n",
      "|Customer Name|TotalSpend|PurchaseFrequency|          AvgSpend|prediction|\n",
      "+-------------+----------+-----------------+------------------+----------+\n",
      "|       Roshan|    298463|              201|1484.8905472636816|         2|\n",
      "|        Vinne|    319565|              203|1574.2118226600985|         0|\n",
      "|        James|    305915|              197|1552.8680203045685|         0|\n",
      "|        Yadav|    273162|              185|1476.5513513513513|         1|\n",
      "|      Willams|    293597|              195| 1505.625641025641|         2|\n",
      "|       Amrish|    333351|              227|1468.5066079295154|         0|\n",
      "|       Sundar|    287151|              187| 1535.566844919786|         2|\n",
      "|         Ravi|    305591|              200|          1527.955|         0|\n",
      "|       Sheeba|    308720|              195| 1583.179487179487|         0|\n",
      "|       Harish|    293839|              208|         1412.6875|         2|\n",
      "+-------------+----------+-----------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import count, avg, sum as _sum\n",
    "\n",
    "# Aggregate customer metrics (TotalSpend, PurchaseFrequency, AvgSpend)\n",
    "customer_data = data.groupBy(\"Customer Name\").agg(\n",
    "    _sum(\"Sales\").alias(\"TotalSpend\"),\n",
    "    count(\"Order ID\").alias(\"PurchaseFrequency\"),\n",
    "    avg(\"Sales\").alias(\"AvgSpend\")\n",
    ")\n",
    "\n",
    "# Prepare data for clustering\n",
    "vector_assembler = VectorAssembler(inputCols=[\"TotalSpend\", \"PurchaseFrequency\", \"AvgSpend\"], outputCol=\"features\")\n",
    "customer_data_features = vector_assembler.transform(customer_data)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(k=3, seed=1)  # 3 clusters\n",
    "model = kmeans.fit(customer_data_features)\n",
    "clusters = model.transform(customer_data_features)\n",
    "\n",
    "# Show clusters\n",
    "clusters.select(\"Customer Name\", \"TotalSpend\", \"PurchaseFrequency\", \"AvgSpend\", \"prediction\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "613f69d0-28e9-47ff-91e5-5ae4bbb0cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal 4\n",
    "#Profit Margin Analysis\n",
    "#We will calculate the profit margin for each Category based on Sales and Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a225ab38-13d5-412a-8d23-a713f3482fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+----------+------------------+\n",
      "|         Category|      TotalProfit|TotalSales|      ProfitMargin|\n",
      "+-----------------+-----------------+----------+------------------+\n",
      "|           Snacks|568178.8500000007|   2237546|25.392946111498965|\n",
      "|Eggs, Meat & Fish|567357.2199999997|   2267401|25.022359079845153|\n",
      "| Fruits & Veggies|530400.3800000008|   2100727|25.248420189772435|\n",
      "|      Food Grains|529162.6400000006|   2115272|25.016292940104183|\n",
      "|           Bakery|528521.0600000002|   2112281|25.021342330873598|\n",
      "|        Beverages|525605.7600000001|   2085313|25.205125561486458|\n",
      "|     Oil & Masala|497895.2900000002|   2038442| 24.42528607632693|\n",
      "+-----------------+-----------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Profit Margin for each product category\n",
    "profit_data = data.groupBy(\"Category\").agg(\n",
    "    _sum(\"Profit\").alias(\"TotalProfit\"),\n",
    "    _sum(\"Sales\").alias(\"TotalSales\")\n",
    ").withColumn(\"ProfitMargin\", col(\"TotalProfit\") / col(\"TotalSales\") * 100)\n",
    "\n",
    "# Show top profitable categories\n",
    "profit_data.orderBy(col(\"TotalProfit\").desc()).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a42a9e50-6c61-407b-a125-ec0faa4df6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+----------+------------------+\n",
      "|         Category|      Sub Category|       TotalProfit|TotalSales|      ProfitMargin|\n",
      "+-----------------+------------------+------------------+----------+------------------+\n",
      "|           Snacks|           Noodles|193685.81000000017|    735435|26.336224139454906|\n",
      "|Eggs, Meat & Fish|              Fish| 147248.0100000001|    560548|26.268581816365433|\n",
      "| Fruits & Veggies|    Organic Fruits|130862.32999999997|    503402|25.995591992085842|\n",
      "|      Food Grains|   Organic Staples|144136.88999999996|    558929|25.788050002773154|\n",
      "|           Bakery|     Breads & Buns|190764.98000000013|    742586| 25.68927774022135|\n",
      "| Fruits & Veggies|Organic Vegetables|133596.37000000002|    520271|25.678227308460404|\n",
      "|      Food Grains|              Rice|126932.35999999997|    498323|25.471904768593856|\n",
      "|        Beverages|     Health Drinks| 267469.7899999999|   1051439|25.438450542542167|\n",
      "|     Oil & Masala| Edible Oil & Ghee| 168593.5800000001|    668086|25.235311022832406|\n",
      "|Eggs, Meat & Fish|              Eggs|144669.91999999987|    575156| 25.15316192476474|\n",
      "+-----------------+------------------+------------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GOAL 5: Profitability Analysis by Product \n",
    "#This will identify the most and least profitable products in terms of their profit margins.\n",
    "profit_by_product = data.groupBy(\"Category\", \"Sub Category\").agg(\n",
    "    _sum(\"Profit\").alias(\"TotalProfit\"),\n",
    "    _sum(\"Sales\").alias(\"TotalSales\")\n",
    ").withColumn(\"ProfitMargin\", col(\"TotalProfit\") / col(\"TotalSales\") * 100)\n",
    "\n",
    "# Show the most profitable products\n",
    "profit_by_product.orderBy(col(\"ProfitMargin\").desc()).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfd178-c0a9-4a62-b70a-eac6cb4d7170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
